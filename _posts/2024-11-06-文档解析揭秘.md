---
layout:       post
title:        "文档解析揭秘：结构化信息提取的技术、挑战与前景"
author:       "mjTree"
header-style: text
catalog:      true
tags:
    - document-parse
---

><small>更新于：2024-11-06</small>

本篇文章是翻译 [**文档解析揭秘：结构化信息提取的技术、挑战与前景**](https://arxiv.org/abs/2410.21169) 论文。  


# 文档解析揭秘：结构化信息提取的技术、挑战与前景
> Qintong Zhang, Victor Shea-Jay Huang, Bin Wang, Junyuan Zhang,
Zhengren Wang, Hao Liang, Shawn Wang, Matthieu Lin, Conghui He, Wentao Zhang  
Shanghai Artifcial Intelligence Laboratory; Peking University; Tsinghua University;  
zhangqintong@pjlab.org.cn,jeix782@gmail.com


## 摘要
文档解析对于将非结构化和半结构化文档（如合同、学术论文和发票）转换为结构化、机器可读数据至关重要。 文档解析从非结构化输入中提取可靠的结构化数据，为众多应用提供了极大的便利。 特别是随着大型语言模型的最新进展，文档解析在知识库构建和训练数据生成中都扮演着不可或缺的角色。 本综述全面回顾了当前文档解析的现状，涵盖了关键方法，从模块化管道系统到由大型视觉语言模型驱动的端到端模型。 详细审查了核心组件，如版式检测、内容提取（包括文本、表格和数学表达式）以及多模态数据集成。 此外，本文还讨论了模块化文档解析系统和视觉语言模型在处理复杂版式、集成多个模块以及识别高密度文本方面所面临的挑战。 它强调了开发更大更丰富的数据集的重要性，并概述了未来的研究方向。

## 1 引言
随着数字化转型加速，电子文档已日益取代纸质文档，成为各行各业信息交换的主要媒介。 这种转变极大地扩展了文档类型的多样性和复杂性，包括合同、发票和学术论文。 因此，对有效管理和检索信息的系统需求不断增长 [1, 2]. 然而，大量历史记录、学术出版物和法律文件仍以扫描或图像格式存在，对信息提取、文档理解和增强检索等任务构成了相当大的挑战 [3, 4, 5].

为了应对这些挑战，文档解析 (DP)，也称为文档内容提取，已成为将非结构化和半结构化文档转换为结构化信息的重要工具。 文档解析识别并提取各种元素，如文本、方程式、表格和图像，来自各种文档输入，同时保留其结构关系。 然后将提取的内容转换为结构化格式，例如 Markdown 或 JSON，从而能够无缝集成到现代工作流程中 [6].

文档解析对于文档相关任务至关重要，它重塑了信息在众多应用程序中的存储、共享和应用方式。 它为各种下游流程奠定了基础，包括在各个实际领域中开发检索增强生成 (RAG) 系统，以及为纸质材料自动构建电子存储和检索库 [7, 8, 9, 10]. 除此之外，文档中还有大量潜在的信息，这些信息在很大程度上尚未得到开发。 文档解析技术可以有效地提取和组织这些丰富的知识，为下一代智能系统的发展奠定坚实的基础。 例如，训练更多更专业、更强大的多模态模型 [5, 11].

然而，近年来，文档解析技术取得了重大进展，特别是基于深度学习的技术，这导致了文档解析工具的激增，并出现了很有希望的文档解析器。 然而，该领域的研究仍然面临着一些局限性。 许多关于文档解析的调查已经过时，导致管道缺乏严谨性和全面性，技术描述无法捕捉到最近的进展和应用场景的变化 [3, 4]. 此外，高质量的评论通常侧重于文档解析中特定的子技术，例如版式分析 [12, 13, 14]、数学表达式识别 [15, 16, 17]、表格结构识别 [18, 19, 20] 和文档中与图表相关的研究 [21]，而没有提供对整个文档解析过程的全面概述。

鉴于这些局限性，迫切需要对文档解析进行全面回顾。 在本次调查中，我们从整体的角度分析了文档解析的进展，为研究人员和开发者提供了对该领域最新发展和未来方向的广泛了解。 本次调查的主要贡献如下：
- 对文档解析的全面回顾。 本文系统地整合和评估了文档解析技术在文档解析管道各个阶段的最新进展。
- 数据集和评估指标的整合。 我们整合了广泛使用的数据集和评估指标，解决了文档解析领域现有综述中的空白。
- 研究人员和实践者的整体洞察。 本工作从整体视角概述了文档解析的现状和未来方向，弥合了学术研究与实际应用之间的差距。
- 新手入门指南。 它作为新手快速了解该领域概况并确定有前景的研究方向的指南。

本文组织结构如下：第 2 节概述了文档解析的两种主要方法。 从第 3 节到第 6.4 节考察了模块化文档解析系统中使用的关键算法。 第 7 节介绍了适用于与文档相关的任务的视觉语言宏模型，重点关注文档解析和 OCR。 第 8 节和第 9 节涵盖了文档解析中的数据集和评估指标。 在第 11 节，我们讨论了该领域的当前挑战并重点介绍了重要的未来方向。 最后，第 12 节提供了简洁且富有洞察力的结论。

![图1：文档解析方法概述](/img/article-img/2024/11/1106_1.jpg)


## 2 方法
文档解析可以大致分为两种方法：模块化管道文档解析系统和基于大型视觉语言模型的端到端方法。

### 2.1 文档解析系统
#### 2.1.1布局分析
布局检测识别文档的结构元素，例如文本块、段落、标题、图像、表格和数学表达式，以及它们的坐标和阅读顺序。 这一基础步骤对于确保准确的内容提取至关重要。 值得注意的是，数学表达式的检测，特别是内联表达式，通常由于其复杂性而被单独处理。

#### 2.1.2 内容提取
- 文本提取：此过程利用光学字符识别 (OCR) 技术将文档图像转换为机器可读文本。 通过分析字符的形状和模式，OCR 可以准确识别和处理图像中包含的文本。
- 数学表达式提取：在此步骤中，检测文档区域内的数学符号和结构，并将它们转换为标准格式，例如 LaTeX 或 MathML。 由于符号及其空间排列的复杂性，这项任务提出了独特的挑战。
- 表格数据和结构提取：表格识别涉及通过识别单元格的布局以及文档图像中行和列之间的关系来检测和解释表格结构。 提取的表格数据通常与 OCR 结果结合起来，并转换为 LaTeX 等格式以供进一步使用。
- 图表识别：此步骤侧重于识别不同类型的图表，并提取基础数据及其结构关系。 图表中的视觉信息被转换为原始数据表或结构化格式，如 JSON。

#### 2.1.3 关系整合
每个步骤都建立在之前的步骤之上，确保从文本到数学表达式、表格和图表无缝衔接，同时利用先进的识别技术将文档内容转换为结构化的机器可读格式。一旦提取了单个内容元素，关系整合将它们组合成一个统一的结构。 此步骤使用布局检测过程中识别的空间坐标，确保元素之间的空间和语义关系得以保留。 基于规则的系统或专门的阅读顺序模型通常用于维护内容的逻辑流程。

### 2.2 端到端方法和多模态大型模型
虽然传统的模块化文档解析系统在特定领域内表现出色，但其架构往往会导致联合优化和跨不同文档类型的泛化方面的局限性。 多模态大型模型，特别是视觉语言模型（VLMs）的最新进展，提供了有希望的替代方案。 GPT-4、Qwen、LLaMA 和 InternVL 等模型可以同时处理视觉和文本数据，便于将文档图像端到端地转换为结构化输出。 由于文档图像带来的独特挑战——例如密集文本、复杂布局和视觉元素的高度可变性——专门的大型模型如 Nougat、Fox 和 GOT 应运而生。 这些模型在自动化文档解析和理解方面迈出了重大步伐。

![图2：文档解析的两种方法](/img/article-img/2024/11/1106_2.jpg)


## 3 布局分析
### 3.1 布局分析技术的介绍
对扫描图像的文档布局分析 (DLA) 的研究始于 1990 年代。 早期的研究集中在简单的文档结构上，通常作为预处理步骤，并且主要使用基于规则的方法 [173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 71, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196] 或统计技术 [197, 198, 178]。

到 2000 年代，DLA 整合了特征工程和机器学习，将任务定义为基于像素的语义分割 [199, 200, 201]。 自 2015 年以来，深度学习技术，特别是卷积神经网络 (CNN) 和 Transformers，已成为该领域的主流，将 DLA 视为像素级分割问题，并利用视觉特征来分析物理布局 [202, 203, 204, 205, 206, 207, 22]。

此外，图卷积网络 (GCNs) 已被用于建模文档组件之间的关系表示 [208, 23, 31, 209, 24, 25]。 基于网格的方法 [210, 211, 212, 26, 27] 强调了保持空间结构的重要性。 最近的研究还将多种数据源集成到这些模型中 [28, 33, 31, 34, 32, 29, 30]。 2020 年前后，多模态自然语言处理 (NLP) 中的自监督预训练影响了 DLA 研究，导致模型联合整合文本和视觉布局信息以进行端到端学习。

![图3：DLA算法概述](/img/article-img/2024/11/1106_3.jpg)

### 3.2 基于视觉特征
早期基于深度学习的 DLA 主要侧重于使用文档图像中的视觉特征来分析物理布局。 文档被视为图像，文本块、图像和表格等元素通过神经网络架构进行检测和提取 [202]。

#### 3.2.1 基于 CNN 的方法
卷积神经网络 (CNN) 的引入标志着 DLA 的重大进步。 这些模型最初是为了目标检测而设计的，后来被改用于页面分割和布局检测等任务。 R-CNN、Fast R-CNN 和 Mask R-CNN 对检测文本块和表格等组件特别有影响 [203]。 后来的研究改进了区域建议过程和架构，以增强页面对象检测 [204]。 像全卷积网络 (FCN) 和 ARU-net 这样的模型被开发用于处理更复杂的布局 [205, 206]。

#### 3.2.2 基于 Transformer 的方法
Transformer 模型的最新进展扩展了它们在 DLA 中的应用。 BEiT（Bidirectional Encoder Representation from Image Transformers），受 BERT 的启发，采用自监督预训练来学习鲁棒的图像表示，在提取全局文档特征方面表现出色，例如标题、段落和表格 [207]。 文档图像 Transformer (DiT) 采用类似 Vision Transformer (ViT) 的架构，将文档图像分割成块，以增强布局分析。 然而，这些模型的计算量很大，需要大量的预训练 [22]。最近的一些工作，例如 [213, 214]，也专注于使用 Transformer 根据文档视觉特征完成分类任务。

#### 3.2.3 基于图的方法
虽然基于图像的方法显著提高了 DLA，但它们往往过于依赖视觉特征，限制了它们对语义结构的理解。 图卷积网络 (GCNs) 通过对文档组件之间的关系建模来解决这个问题，增强了对布局的语义分析 [208, 23, 31]。 例如，Doc-GCN 提高了对布局组件之间语义和上下文关系的理解 [24]。 GLAM 是另一个突出的模型，它将文档页面表示为结构化图，将视觉特征与嵌入式元数据相结合，以获得更高的性能 [25]。

#### 3.2.4 基于网格的方法
基于网格的方法通过将文档布局表示为网格来保留空间信息，这有助于保留空间细节 [210, 211, 212, 26, 27]。 例如，BERTGrid 采用 BERT 来表示布局，同时保持空间结构 [26]。 VGT 模型集成了 Vision Transformer (ViT) 和 Grid Transformer (GiT) 模块，以在符元和段落级别捕获特征。 然而，基于网格的方法往往面临着参数量大、推理速度慢等挑战，限制了它们的实际应用 [27]。

### 3.3 与语义信息集成
随着文档分析变得更加复杂，仅仅进行物理布局分析已经不足够。 尽管有研究表明，在某些基于字母的小语种中，诸如 YOLO v8 等优秀的物体检测模型在文档布局分析方面仍然处于领先地位 [215]，并且相关改进也已取得 [216]。 ，结合语义信息的 DLA 方法仍然是一个重要的发展方向。 逻辑布局分析需要根据语义角色对文档元素进行分类，例如标题、图表或页脚。 随着多模态模型的兴起，结合视觉、文本和布局信息的方法在 DLA 研究中越来越突出。

逻辑布局分析，源于根据语义角色对文档元素进行分类的需求，促使了多模态模型的开发，这些模型整合文本和布局信息以进行更全面的分析。 研究人员通过将监督学习与预训练的自然语言处理 (NLP) 或计算机视觉 (CV) 模型相结合，探索了多模态数据整合。 例如，LayoutLM 是第一个在单个框架内融合文本和布局信息的模型，它使用 BERT 架构通过文本、位置和图像嵌入来捕获文档特征 [28]。

[33] 通过将 RoBERTa 与 GCN 相结合来扩展此功能，以从文本和图像中捕获关系布局信息。 [31] 引入了一个多尺度自适应聚合模块来融合视觉和语义特征，从而生成一个注意力图，以实现更准确的特征对齐。

多模态 NLP 中的自监督预训练也极大地推动了该领域的发展。 在预训练过程中，模型使用统一的 Transformer 架构联合处理文本、图像和布局信息，使它们能够从各种文档类型中学习跨模态知识。 这种方法提高了模型的通用性，只需最少的监督即可跨不同文档类型和样式进行微调。

2020 年，[34] 提出了一个多模态文档预训练框架，该框架对来自多页文档的信息进行端到端编码，并包含文档主题建模和随机文档预测等任务。 该框架使模型能够学习图像、文本和布局的丰富表示。 UniDoc [32] 等值得注意的工作使用 Transformer 和 ResNet-50 架构来提取语言和视觉特征，并通过门控跨模态注意力机制进行对齐。

进步包括 LayoutLMv2 和 LayoutLMv3，它们通过优化文本、图像和布局信息的融合来改进 LayoutLM。 这些模型通过更深层的多模态交互和掩蔽机制来改进特征提取，从而实现更高效、更全面的文档分析 [29, 30]。此外，LayoutLLM [35] 试图使用大型语言模型来整合某些语义信息，以完成与文档布局相关的任务。


## 4 光学字符识别
### 4.1 文档 OCR 简介
光学字符识别 (OCR) 历史悠久，起源于计算机的早期发展。 该概念最初由 Tausheck 于 1929 年提出。 如今，OCR 是计算机视觉和模式识别领域的关键研究方向，旨在识别视觉数据中的文本，并将其转换为可编辑的数字格式，以便进行后续分析和组织。

在 20 世纪 50 年代和 60 年代，OCR 研究集中在手写文档识别上，例如支票处理和邮件排序。 在这一时期，OCR 系统主要采用预处理技术和基于规则或模板匹配的方法。 例如，ABBYY OCR 的早期版本采用了图像二值化、降噪和布局分析，通过模板匹配来识别字符。

在深度学习出现之前，OCR 系统主要依靠特征工程和传统的机器学习技术来识别字符。 这些方法通常应用于邮政编码识别、表单处理和银行业务等任务。 一个值得注意的例子是 Tesseract OCR，它由惠普实验室于 1984 年开发，在其早期版本 (4.x 版本之前) 中使用这些技术。

随着 OCR 技术被集成到各个行业，人们对更高精度和更广泛的适用性的需求不断增加。 研究人员此后探索了更高级的 OCR 应用，包括场景文本识别、多语言识别和文档字符识别。 自 2010 年以来，端到端深度学习算法的发展极大地改变了 OCR，提高了其效率和应用范围。

OCR 通常涉及两个主要阶段：文本检测和文本识别。 首先，文本在图像中被定位，然后识别算法被应用于将识别的文本转换为计算机可读的字符。 当 OCR 集成文本检测和识别时，它被称为文本识别。 本节讨论了 OCR 的这三个关键技术方面。

![图4：OCR 算法概述](/img/article-img/2024/11/1106_4.jpg)

### 4.2 文本检测
传统的非深度学习文本检测算法通常在具有高对比度背景的简单场景中有效。 但是，它们通常需要手动调整参数才能在不同的上下文中获得最佳性能，从而限制了它们的泛化能力。 相比之下，基于深度学习的文本检测算法改进了目标检测和实例分割技术，可以分为四种主要方法：单阶段回归方法、两阶段区域建议方法、基于实例分割的方法和混合方法。

#### 4.2.1 基于回归的单阶段方法
基于回归的方法，也称为直接回归方法，直接从图像中的特定点预测文本框的角坐标或纵横比，从而绕过了多阶段候选区域生成和后续分类的需要。 像 YOLO 和 SSD 这样的算法已被用于文本检测，并进行了修改以处理文本特有的挑战，例如不同的纵横比和方向 [36, 37]。 例如，CTPN [38] 通过回归垂直位置和横向偏移来实现精确的文本行定位。 SegLink [217] 和 DRRG [39] 等方法采用回归技术来处理不规则文本形状，而傅立叶变换 [40] 能够对复杂的文本轮廓进行紧凑的表示。 虽然基于回归的方法在计算上是高效的，并且与深度学习模型很好地集成，但它们可能难以处理模糊的边缘和杂乱的背景。

#### 4.2.2 基于区域建议的两阶段方法
基于区域建议的方法将文本块视为特定的检测目标，并使用两阶段目标检测技术（如 Fast R-CNN 和 Faster R-CNN）对其进行处理。 这些方法旨在生成针对文本优化的候选框，并提高对任意方向文本的检测精度。 例如，DenseBox [218] 引入了一个端到端的全卷积网络 (FCN) 框架，该框架通过多任务学习将位置和尺度信息整合在一起，从而提高了检测精度。 同样，DeepText [41] 引入了一个 Inception-RPN 来生成更详细的文本候选框。 虽然 Faster R-CNN 主要针对水平文本设计，但一些研究已经增强了它检测不规则文本区域的能力 [42, 43, 44, 45, 46]。 由 [47] 进行的研究将这些方法扩展到处理任何方向的文本，包括弯曲的文本块，进一步提高了它们的鲁棒性。

#### 4.2.3 基于分割的方法
文本检测也可以被视为图像分割问题，其中像素被分类以识别文本区域。 这种方法为处理各种文本形状和方向提供了灵活性。 早期的方法 [48] 使用全卷积网络 (FCN) 来检测文本行，而像 PAN [49] 这样的后期算法提高了效率和精度。 CRAFT [50] 代表着一次重大进步，它采用了字符级检测，消除了对大型感受野的需求。 像 [48] 这样的实例分割方法通过将每个块视为一个独立的实例来解决紧邻文本块等挑战。 SPCNET [51] 和 LSAE [52] 等技术分别使用金字塔注意模块和双分支架构进一步改进了这种方法。 后处理（如二值化）在基于分割的方法中至关重要，可微二值化 (DB) [53] 通过将二值化集成到网络中来提高检测速度和精度。

#### 4.2.4 混合方法
混合方法结合了回归和分割方法的优势，以捕捉全局和局部文本细节，从而提高定位精度，同时减少对大量后处理的需求。 诸如 EAST [54] 和 MOST 等技术结合了位置感知非极大值抑制 (PA-NMS) 来优化跨不同尺度的检测。 最近的方法，如 CentripetalText [55]，使用向心偏移来细化文本定位。 此外，图网络和 Transformer 架构等创新 [56, 57] 通过利用自适应边界提议和注意力机制进一步增强了检测能力。 迁移学习和多模态集成方面的进展 [58, 59]，特别是在基于 Transformer 的架构中，通过整合视觉文本表示，解决了检测小文本区域的挑战并提高了准确性。

总之，文本检测已取得显著进展，利用了目标检测、分割和新颖架构创新的改进，使其成为各种应用的强大工具。

### 4.3 文本识别
文本识别是光学字符识别 (OCR) 的一个关键组成部分，指的是将书写或印刷文本的图像自动转换为机器可读格式的过程。 文本识别系统的首要目标是从视觉数据中解释字符和单词，以便进行后续的计算任务。 随着时间的推移，出现了各种文本识别方法，主要分为三类：基于视觉特征的方法、基于连接主义时间分类 (CTC) 损失的方法和序列到序列 (seq2seq) 技术。

#### 4.3.1 基于视觉特征的 OCR 技术
- 基于图像特征的方法：OCR 技术的最新进展利用图像处理，特别是卷积神经网络 (CNN)，从文本图像中捕获空间特征。 这些方法通过消除对传统特征工程的需求，直接从图像中提取特征来定位和识别字符。 这简化了模型设计和实现，同时有效地捕获空间结构信息，使这些技术特别适用于规则或半结构化文本图像。
例如，[219] 提出了一个使用 CNN 来检测文本区域和分类字符的模型，有效地管理字符空间排列。 同样，[60] 引入了一个合成数据生成器，并结合深度 CNN 架构来提高跨不同文本类型的适应性。 CA-FAN 模型 [61] 通过采用字符注意力机制来提高字符识别准确率。 此外，TextScanner [62] 将 CNN 与循环神经网络 (RNN) 相结合，以提高字符分割和定位精度。 尽管这些方法有效，但在处理复杂或不规则文本时，尤其是在存在大量背景噪声或复杂文本结构的情况下，它们面临着挑战，通常需要额外的后处理以提高识别精度。

- 基于 CTC 损失的方法：连接主义时间分类 (CTC) 损失函数通过使模型能够在训练期间优化输入和输出序列之间的显式对齐来解决序列对齐问题。 CTC 计算所有可能对齐路径上的概率，使其特别适合处理可变长度的文本。
CTC 的一个显着应用是 [63] 的 CRNN 模型，该模型将 CNN 和 RNN 架构与 CTC 损失相结合，用于序列生成。 Deep TextSpotter [64] 将 CNN 特征提取与 CTC 相结合，以提高文本检测和识别精度。 ADOCRNet [65] 进一步将 CTC 与 CNN 和双向长短期记忆 (BLSTM) 网络相结合，用于阿拉伯语文档识别。 然而，CTC 在处理扩展文本和上下文细微差别方面存在困难，这会增加计算复杂度并影响模型训练效率和实时性能。

- 序列到序列方法：序列到序列 (seq2seq) 技术使用编码器-解码器架构来编码输入序列并生成相应的输出。 这些方法通过注意力机制管理输入和输出序列之间的长距离依赖关系，促进端到端训练。 传统方法通常使用 RNN 和 CNN 将图像特征转换为一维序列，然后由基于注意力的解码器进行处理。 尽管它们有效，但将图像转换为一维序列以用于基于 Transformer 的架构存在挑战，特别是在处理任意方向和不规则文本时。
为了解决这些问题，模型使用输入校正和二维特征图等策略。 例如，空间变换网络 (STNs) 将文本图像校正为矩形、水平对齐的字符，如 ASTER [66]、ESIR [68] 和 MORAN [66] 中所示。 其他模型通过学习 2D 特征图来直接从 2D 空间提取字符，以避免输入修改，从而适应不规则和多方向文本，如 SAR [70]、AON [67] 和 SATRN [72] 所示。 Transformer 架构的兴起代表着从传统的 CNN 和 RNN 模型向基于注意力的编码器-解码器系统转变。 例如，NRTR [69] 采用完全自注意力架构，使用卷积层将 2D 输入图像转换为 1D 序列，用于编码器-解码器框架。 像 ViTSTR [71] 这样的视觉 Transformer 模型摒弃了传统的骨干网络，专门使用视觉 Transformer (ViT) 架构进行编码，而 TrOCR [73] 完全依赖 Transformer 架构进行图像处理和文本生成，完全避免了 CNN。

不规则或细长文本序列的性能改进侧重于更好地处理二维几何位置信息。 例如，[220] 提出的方法集成了类似于传统 R-CNN 方法的校正模块，以及文本分组和排列模块。 LOCR [74] 通过将文档元素的位置信息与图像块的位置编码结合起来，提高了文档中长文本的 OCR 性能。 OCR 研究仍在不断发展，尤其是在使用 Transformer 架构来提高复杂图像文本的性能方面 [221, 81]。

#### 4.3.2语义信息的整合
文本识别传统上被视为一种视觉分类任务，从语义信息的整合和上下文理解中获益匪浅，尤其是在处理不规则、模糊或遮挡的文本时。 最近的研究强调将语义理解纳入文本识别系统，主要分为三种方法：字符级语义整合、通过专用语义模块增强和训练改进以提高上下文感知能力。
- 字符级语义整合： 通过利用字符相关特征，如数量和顺序，增强 OCR 性能与字符级语义信息。 [75] 提出的 RF-L (互惠特征学习) 框架突出了使用隐式标签（如文本长度）来提高识别的益处。 RF-L 整合了一个计数任务 (CNT) 来预测字符频率，从而帮助识别任务。 同样，[76] 提出了一种上下文感知双平行编码器 (CDDP)，使用交叉注意力机制和专门的损失函数来整合排序和计数模块。 尽管通过整合字符信息提高了性能，但在从大规模无标注文本中获取用于语言模型预训练的各种先验知识方面仍然存在挑战。
- 通过语义模块的增强: 虽然字符级语义整合很有价值，但一些方法专注于独立的语义模块来捕获更高层次的语义特征。 这些策略通过专门模块内的上下文关系来对齐视觉和语义数据。 例如，SRN [77] 引入了一个并行视觉注意力模块 (PVAM) 和一个全局语义推理模块 (GSRM) 来将 2D 视觉特征与字符对齐，将字符特征转换为用于全局推理的语义嵌入。 同样，SEED [78] 在编码器和解码器之间添加了一个语义模块，通过语义转换增强特征序列。 ABINet [79] 使用单独训练的语言模型进行上下文细化，通过迭代反馈来细化字符位置。 这些策略有效地对齐语义和视觉数据，但在充分利用语义关系方面仍然存在挑战。
- 上下文感知的训练改进: 从自然语言处理 (NLP) 中改编的预训练策略，例如 BERT，在增强 OCR 任务中的上下文感知方面发挥了关键作用。 像 VisionLAN [80] 这样的方法使用掩码来提高上下文理解，引入了掩码语言感知模块 (MLM) 和视觉推理模块 (VRM) 用于并行推理。 同样，Text-DIAE [81] 在预训练期间应用降级方法，例如掩码、模糊和噪声添加，以提高 OCR 能力。 PARSeq [82] 修改了置换语言建模 (PLM)，通过重新排序编码的标签来增强文本识别，以获得更好的上下文序列。 虽然这些预训练方法提高了语义学习，但它们通常会增加计算复杂度和资源需求。

### 4.4文本定位
文本识别涉及从图像中检测和转录文本信息，包括文本检测和识别任务。 传统上，这些任务是独立处理的，检测器识别文本区域，识别模块随后转录识别出的文本。 尽管这种方法很简单，但检测和识别的单独处理可能会影响性能，因为最终结果的有效性很大程度上取决于文本检测模型的准确性。 随着深度学习的进步，最近的努力越来越集中于开发端到端模型，这些模型将文本检测和识别集成在一起，从而通过共享特征表示来提高效率和准确性。 基于深度学习的端到端文本识别模型主要分为两类：两阶段方法和单阶段方法，每种方法都有其独特的优势。 虽然两种方法都得到了探索，但最近的研究主要强调单阶段方法。

**两阶段方法：**  
两阶段方法整合了文本检测和识别架构，实现了联合训练和特征对齐。 这种方法允许通过提取共同特征（通常通过共享卷积层）在检测和识别任务之间共享信息，并使用感兴趣区域 (RoI) 机制将任务联系起来。 在检测阶段，模型识别潜在的文本区域并将它们映射到识别阶段的共享特征图中进行转录。  

例如，一种基础的两阶段方法将单扫描文本检测器与使用矩形 RoI 的序列到序列识别器结合在一起 [222]。 后续工作使用类似的架构改进了多方向文本检测 [64]。 然而，矩形 RoI 最适合有组织的文本布局，可能会受到背景元素的损害，促使研究人员探索替代 RoI 方法。 一些方法采用了目标检测技术，例如 FOTS [223] 与 RoIRotate 机制，以及 Mask TextSpotter 系列 [224, 225]、AE TextSpotter [226] 和 ABINet++ [227] 使用 RoIAlign。 值得注意的是，Mask TextSpotter v1 是第一个完全实现端到端 OCR 的方法，它允许在联合训练期间在检测和识别之间进行反馈，而 Mask TextSpotter v3 [225] 引入了分割提案网络 (SPN) 来提供灵活的文本区域表示。

其他两阶段方法，例如 [228]，将注意力机制与文本对齐层而不是 RoI 整合在一起。 RoI 机制方面的创新包括 TextDragon 的 [229] RoLSide 运算符，它提取并对齐任意文本区域，以及 ABCNet 的 BezierAlign [230]，它适应文本轮廓而不是矩形边界。 PAN++ [231] 使用掩蔽的兴趣区域注意力识别头来平衡准确性和速度，而 SwinTextSpotter [232] 引入了一种检测感知识别的机制。 2022 年，GLASS [233] 提出了旋转 RoIAlign，以增强从共享主干提取的文本特征，通过全局注意力模块解决文本大小和方向变化带来的挑战。

尽管取得了这些创新，但两阶段方法仍存在固有的局限性。 它们对精确检测结果的依赖加大了对检测模块的要求，并需要高质量的标注数据集。 此外，RoI 操作和后处理步骤计算量很大，特别是对于任意文本形状。

**一阶段方法：**  
一阶段方法将文本检测和识别统一在一个架构中，无需独立的模块。 通过共享损失函数，这两个任务可以共同进行训练和优化，避免模块分离可能导致的性能损失。 第一个一阶段方法由 [234] 提出，引入了卷积字符网络，该网络将字符检测为基本单元，并生成字符边界和标签，无需 RoI 裁剪。 尽管对英文文本有效，但这种方法计算量很大。 CRAFTS [235] 继续这种基于字符的方法，将检测结果集成到基于注意力的识别器中，以在整个网络中传播识别损失。

随后的发展，如 [236]，结合了形状转换器模块来优化端到端的检测和识别，而 MANGO [237] 采用了一种位置感知掩蔽注意力模块，将注意力权重直接应用于字符序列。 最近的编码器-解码器模型进一步发展，PGNet [238] 和 PageNet [239] 将特征图解码为序列，而 SPTS 系列 [240, 241] 和 TESTR [242] 采用了基于 Transformer 的架构。 基于 CLIP 的模型 [243] 通过增强交叉注意力机制，改进了图像和文本嵌入之间的协作。 在 [244] 中，文本识别在视频文本上的应用被引入，TransDETR 是一个基于 Transformer 的框架，简化了文本随时间的跟踪和识别，这可能有利于文档文本识别任务。

尽管单阶段模型展示了多功能性和提高的准确性，但它们需要比两阶段模型更复杂的训练过程，并且可能在某些专门的文本处理任务中不能同样有效地执行。


## 5 数学表达式检测与识别
### 5.1 数学表达式检测与识别简介
数学表达式检测与识别侧重于识别和解释文档中的数学表达式。 此过程通常按数学表达式的类型进行分类：手写或印刷。 手写数学表达式，得益于触控笔技术的进步，可以进一步划分为在线（实时）和离线识别；本讨论专门针对离线数学表达式。

在文档中，数学表达式表现为显示的数学表达式，它们不同于普通文本，或嵌入文本行中的内联表达式。 显示的数学表达式更容易使用文档布局分析来识别，而内联数学表达式由于其与普通文本的接近性而带来了挑战，需要专门的检测技术。

早期数学表达式检测方法依赖于基于规则的方法 [245, 246, 247, 248, 249, 250, 251, 252, 253] 或文档布局分析的改编 [254, 255, 256]。 对于内联数学表达式，统计和机器学习技术，包括支持向量机和贝叶斯模型，通常用于特征提取和分类 [252, 253, 257, 256, 258]。

随着深度学习的出现，数学表达式检测越来越像文档图像中的目标检测，利用边界框或实例分割来隔离数学表达式区域。 连续的数学表达式块可以通过高级分割技术进行管理。

识别印刷数学表达式的挑战首先在 1960 年代得到解决 [259]，标志着将数学表达式图像转换为结构化代码或标签的努力的开始。 与普通文本不同，数学表达式由于其广泛的符号集、二维格式和上下文相关的含义而带来了独特的挑战。 到 20 世纪末，数学表达式识别已被细分为符号识别和结构分割 [260]。 顺序方法成为了一种关键方法，在通过分类算法识别之前对数学表达式中的符号进行分割 [261, 262, 263, 264]，辅以传达数学表达式 2D 结构的语法规则 [265, 266, 267]。

从另一个角度来看，数学表达式识别 (MER) 可以被视为一个全局优化任务，它以整体的方式识别符号及其相互关系，而无需显式分割 [268, 269, 270]。

随着基于序列的深度学习在机器翻译等领域的成功，PAL 模型将序列到序列的“编码器-解码器”架构引入数学表达式识别，这代表着一个重大进步 [92]。 尽管后续模型在此基础上进行了构建，但仍然存在数学表达式检测和识别的端到端解决方案的全面短缺，这导致了一些独特的挑战，本文将对此进行探讨。

![图5: 数学表达式检测和识别的概述](/img/article-img/2024/11/1106_5.jpg)

### 5.2 数学表达式检测
#### 5.2.1 早期工作和卷积神经网络
数学表达式检测 (MED) 的早期尝试利用卷积神经网络 (CNN) 进行数学表达式定位。 [271, 204, 272] 等研究利用 CNN 以及传统的特征提取方法来生成数学表达式识别的边界框。 值得注意的是，[271] 使用循环神经网络 (RNN) 来提取字符序列；但是，这些模型不支持完全的端到端检测，这限制了它们的泛化能力和性能。 Unet 模型在 [83] 中引入用于端到端检测，其重点在于印刷文件，并避开了复杂的分割任务。 虽然它在检测行内数学表达式方面有效，但它缺乏对噪声的鲁棒性。

#### 5.2.2 目标检测算法的进展
MED 通过将通用目标检测算法改编为专门形式而不断发展，包括单阶段和两阶段方法。 单阶段检测器，例如 DS-YOLOv5 [90]，结合了可变形卷积和多尺度架构来提高检测精度和速度。 同样，单发多盒检测器 (SSD) [85] 使用滑动窗口策略加速计算，以进行尺度不变检测。 2021 年 ICDAR 竞赛展示了诸如广义焦点损失 (GFL) 之类的进步，以解决类别不平衡问题，并通过特征金字塔网络增强了小数学表达式检测。

两阶段检测器，特别是 R-CNN 变体 [87]，提供了很高的精度，但以计算速度为代价。 Faster R-CNN 和 Mask R-CNN 等技术直接应用，并通过区域建议网络 (RPN) 进行细化，以提高性能 [273, 274]。 尽管与多锚配置相关的挑战仍然存在，但 FCOS 和 DenseBox 等无锚方法已经出现，尽管它们缺乏针对 MED 的特定优化 [275, 276]。

#### 5.2.3 实例分割技术
实例分割算法与 MED 非常契合，通过像素级分割有效地管理非线性、密集的数学表达式配置。 Mask R-CNN [277] 通过在其框架中加入像素掩码预测，将该领域推向了新的高度，从而实现卓越的区域识别。 PANet [278] 和混合任务级联 (HTC) [279] 通过改进语义定位并将检测与分割任务相结合，进一步增强了这些方法。 在 2024 年，FormulaDet [91] 通过将 MED 构造成一个实体和关系提取问题，成功地利用了上下文感知和布局感知网络，从而进行了创新。 这种集成方法证明了在理解和检测复杂公式结构方面取得了重大进展。

### 5.3 数学表达式识别 (MER)
数学表达式识别 (MER) 模型通常使用编码器-解码器架构将视觉表示转换为 LaTeX 等结构化格式。 这些模型主要依赖于基于 CNN 的编码器，最近的进展整合了基于 Transformer 的编码器。 在解码器方面，通常采用 RNN 和 Transformer 架构，并且许多增强功能提高了模型性能。

#### 5.3.1 MER 中的编码器策略
MER 编码器的基本任务是提取有意义的图像特征，这些特征可以概括数学表达式的复杂性。 传统卷积神经网络 (CNN) 以其捕获局部特征的能力而闻名，已被广泛使用；然而，它们通常难以应对数学表达式表示的多尺度和复杂性。 诸如密集卷积架构和多方向扫描 (例如，MDLSTM) 等增强功能通过促进丰富的空间依赖性来解决这些局限性。
- 卷积方法： 已经提出了各种卷积架构，包括 DenseNet 和 ResNet，以改进 MER 的特征提取 [94, 95]。 后续发展包括用 RNN 或位置编码增强 CNN，以更好地捕获数学表达式结构，从而增强空间和上下文解释 [92, 93]。
- 变换器编码器： 认识到 CNN 在处理长距离依赖方面的局限性，较新的模型使用基于视觉的变换器，例如 Swin Transformer [96]，它们通过自注意力机制提供管理全局上下文和复杂性的优越能力。

#### 5.3.2 MER 的解码器方法
MER 中的解码涉及类似于光学字符识别 (OCR) 的顺序数据处理，使用诸如 RNN 和 Transformer 等架构。 基于 RNN 的解码器通过注意力机制增强，生成与输入的固有顺序相对应的序列 [92, 97]。 这些模型在管理上下文依赖性方面表现出色，这对于准确地解决嵌套和分层表达式至关重要。

先进的设计结合了门控循环单元 (GRU) 和注意力机制，以实现资源高效的处理，满足复杂数学表达式结构的复杂性。 同时，树形和基于 Transformer 的解码器解决了与消失梯度和计算开销相关的挑战，从而增强了处理大量公式符号的鲁棒性 [98, 99]。

#### 5.3.3 其他改进策略
除了编码器-解码器架构的改进之外，还出现了其他策略来提高 MER 的准确性。
- 字符和长度提示： 将字符和长度信息整合有助于管理不同的手写风格和序列长度，这些信息通常作为传统框架中的补充线索嵌入 [100, 101]。
- 笔画顺序信息： 利用笔画序列数据对于在线手写数学表达式特别有利，提供了对结构语义的更深入的见解 [102, 103]。
- 数据增强： 创新型数据操作技术，例如模式生成和预训练增强，对于增强数据集鲁棒性和模型性能至关重要，从而减轻架构停滞 [93, 96]。


## 6 表格检测和识别
### 6.1 表格检测和识别的介绍
表格是各种文档中必不可少的信息载体，包括报告、学术论文、财务报表和技术文档。 它们以结构化和连贯的方式呈现数据，便于快速理解关系和层次结构。 因此，准确的表格检测和识别对于有效的文档分析至关重要。

表格检测包括在文档图像或电子文件中识别和分割表格区域。 此过程旨在定位表格，同时将其与其他内容（如文本或图像）区分开来。 表格检测的精度直接影响后续结构识别和数据提取的成功，使其成为至关重要的初始步骤。

当前表格检测方法主要依赖于文档布局分析 (DLA) 和增强的目标检测算法。 虽然 DLA 在识别和分割表格区域方面表现出色，但它通常难以处理缺乏明显边界的表格。 基于 Transformer 的目标检测算法也已用于表格检测，对通用方法进行改进以获得更好的结果。

随着检测精度的提高，研究重点已转向表格结构识别，这涉及分析检测后表格的内部结构。 这包括诸如分割行和列、提取单元格内容以及将单元格关系解释为结构化格式（如 LaTeX）之类的任务。 增强识别能力有助于自动化信息处理，最大限度地减少人工干预并丰富文档分析应用程序。

本节回顾了基于目标检测的表格检测算法，并讨论了从最近研究中得出的三种基于深度学习的表格识别方法。

![图6：表格检测和识别概述](/img/article-img/2024/11/1106_6.jpg)

### 6.2 基于目标检测算法的表格检测
表格检测 (TD) 通常被视为一项目标检测任务，其中表格被视为对象，并且最初为自然图像开发的模型被应用。 尽管页面元素和自然图像之间存在固有差异，但单阶段、两阶段和基于 Transformer 的模型可以通过仔细的重新训练和调优产生稳健的结果，通常作为 TD 的基准。

为了使目标检测适应 TD，许多研究都试图改进标准方法。 例如，[104] 将 PDF 特征（如字符坐标）集成到基于 CNN 的模型中。 [105] 通过修改文档图像的表示形式并优化锚点，为文档图像定制了 Faster R-CNN。 [106] 将可变形 CNN 与 Faster R-CNN 相结合，以适应不同的表格规模，而 [107] 专门针对表格对 Faster R-CNN 进行微调。 [108] 使用了 YOLO 系列，增强了锚点和后处理技术。

为了解决表格稀疏性问题，[109] 使用高斯噪声增强图像大小提案和多对一标签分配扩展了 SparseR-CNN，引入了信息覆盖评分 (ICS) 来评估识别精度。

### 6.3 表格结构识别
历史上，表格结构识别依赖于手动规则和启发式方法，例如用于线检测的霍夫变换和用于无框表格的空白空间分析。 然而，这些方法难以处理复杂的表格布局。 最近的进展利用了文档布局和公式检测中的算法来改进表格结构识别，将方法归类为行和列分割、单元格检测和序列生成方法。 TabNet [280] 是一种创新的深度学习模型，专为表格特征提取而设计，以端到端的方式处理表格数据的数值和分类特征。 它引入了一个高效且可解释的学习架构，针对各种下游任务进行了优化。 TabNet 的顺序注意机制允许模型通过实例级稀疏特征选择和多步决策架构，逐步选择性地关注相关特征。 这种方法增强了 TabNet 在局部和全局级别上解释特征重要性的能力。 在此基础上，TabTransformer [281] 等模型进一步推进了表格特征提取，为开发强大的表格识别模型提供了宝贵的见解。

#### 6.3.1 基于行和列分割的方法
表格结构识别中的主要挑战通常在于检测单个单元格，特别是由于存在大面积空白。 早期的深度学习方法通过将表格分割成行和列来解决这个问题。 这些算法通常遵循自上而下的策略，首先识别整体表格区域，然后将其分割成行和列。 这种相对简单的方案对于边界清晰且布局简单的表格来说是有效的。
- 行和列检测： 最初，表格结构识别被认为是表格检测的扩展，主要利用目标检测算法来识别表格边界框。 然后，分割算法建立了行和列之间的关系。 卷积神经网络 (CNN) 和 Transformer 架构在这个背景下发挥了至关重要的作用。 例如，Faster R-CNN 被用于表格检测，随后是用于语义分割的全卷积网络 (FCN)，有效地捕获了表格的结构 [110]。 同样，Unet 和 DeepLab 等模型促进了端到端的语义分割，用于像素级识别 [111]。 基于 CNN 的技术，例如使用 VGG-19 的多任务学习，专注于表格区域和行/列分割 [112]。 相反，Transformer，如 DETR，擅长识别图像中的全局关系，从而增强泛化能力。 创新包括通过 Transformer 查询进行行和列分割 [282] 以及动态查询增强模型 DQ-DETR [113]。 此外，双向门控循环单元 (Bi-GRUs) 通过双向扫描图像有效地捕获了行和列分隔符 [114]。
- 融合模块： 虽然早期方法强调检测表格线，但它们往往忽略了复杂的单元间关系。 先进的算法构建了模型来估计单元之间合并的概率，从而提高了在缺乏明确行和列线的表格中的识别精度。 例如，嵌入模块被用于将纯文本集成到网格上下文中，通过 GRU 解码器引导合并预测 [115]。 其他技术已经利用邻接标准和空间兼容性来预测单元合并 [116]。 全局计算模型（如 Transformer）的集成进一步增强了对复杂表格的分析 [117]。

CNN 仍然是表格图像特征提取的基础，尽管最近的努力旨在针对表格特有的特征优化架构。 例如，用 ShuffleNetv2 替换 ResNet18 显着减少了模型参数 [283]。 尽管取得了进展，但在缺乏明确线条的表格中（例如内容稀疏或排列不规则的表格）仍然存在挑战。

#### 6.3.2 基于单元的方法
基于单元的方法，被描述为自下而上的方法，通过检测单个单元格并根据视觉或文本关系合并它们来构建表格。 这些方法通常涉及两个阶段：检测单元格边界，然后将单元格关联起来形成整个表格结构，从而在处理复杂表格和最小化错误传播方面具有优势。

早期的改进集中在提高单元格关键点检测和分割精度。 例如，HRNet 作为多阶段实例分割任务中高分辨率特征表示的骨干网络 [118]。 一些方法引入了新的损失项来增强检测，包括连续性和重叠损失 [119]。 其他人开发了双路径模型来学习局部特征并优化表格分割 [120]。

顶点预测，它专注于单元格的角点，被证明有利于解决由角度或透视导致的变形单元格。 循环配对模块等技术同时预测单元格的中心和顶点 [121]。 将表格表示为图结构，能够更细致地理解，使用图神经网络 (GNN) 来模拟复杂的关系 [122]。 这些方法有效地克服了传统基于网格的方法在捕获复杂的单元格关系方面的局限性。

基于图的方法通过将表格视为图来有效地利用单元格特征，其中单元格代表顶点，关系代表边。 这种方法允许对邻接关系进行全面建模，将 GNN 作为管理复杂表格的强大工具 [123]。

虽然有效，但基于单元格的方法在计算上可能很费力，因为它们涉及对每个单元格进行独立检测和分类。 发生在这个阶段的错误会严重影响最终的表格结构。

#### 6.3.3 图像到序列
基于 OCR 和公式识别方面的进展，图像到序列方法将表格图像转换为结构化格式，例如 LaTeX、HTML 或 Markdown。 编码器-解码器框架利用注意力机制将表格图像编码为特征向量，解码器随后将这些特征向量转换为描述性文本序列。

[124] 的早期工作实现了编码器-解码器架构，用于将科学论文中的图像翻译成 LaTeX 代码。 后续模型通过双解码器架构改进了这些技术，使结构和文本信息能够同时处理 [125]。 MASTER 架构适用于场景文本识别，有效地区分了结构元素和位置信息 [126]。

最近的进展提出了专门为科学表格设计 Transformer 架构，增强了对特定环境（例如医疗报告）中复杂特征的鲁棒性 [127]。 VAST 框架等解决方案通过使用双解码器来管理 HTML 和坐标序列，证明了准确性的提高 [128]。

虽然这些方法在处理复杂表格方面具有相当大的优势，但固有的挑战在于训练模型以充分捕获不同的表格结构，而不会屈服于错误传播。

![图7：文档中与图表相关的任务概述](/img/article-img/2024/11/1106_7.jpg)

### 6.4 图表感知
#### 6.4.1 文档中图表相关任务介绍
文档中的图表是图形表示，以简洁直观的方式呈现数据，使人们更容易可视化模式、趋势和关系。 常见的图表类型包括折线图、柱状图、面积图、饼图和散点图，它们都在传达关键见解方面发挥着至关重要的作用。

与处理文档中的图表相关的任务通常涉及几个子任务，例如图表分类、复合图表的分割、标题匹配、图表元素识别以及数据和结构提取。 第一步，图表分类，对于区分不同的图表类型至关重要，因为每种类型都需要独特的提取方法。 例如，识别转折点和端点等关键点对于折线图至关重要，而在柱状图中，数据和相关的文本标签都是关键组成部分。 确定图表是折线图、柱状图还是其他类型对于成功提取至关重要。

一旦分类，标题将与图表匹配，组合图表将被分割。 轴、标签、刻度、数据点和图例等元素通常使用边界框检测进行检测和识别，以方便进一步提取。

图表感知，也称为图表信息提取，涉及从视觉表示中检索数值和文本数据。 最终目标是将视觉数据转换为表格或 JSON 等结构化格式，使模型能够更有效地进行处理。 例如，散点图点、折线图拐点、饼图比例和条形图元素被提取出来并与相应的标签匹配，以生成结构化数据。 此外，对于流程图、结构图和思维导图等图表，提取结构信息有助于推理等后续任务。

在图表理解和推理任务（包括摘要和问答）中使用视觉语言模型，为用自然语言解释和表示图表内容提供了巨大潜力 [284].

### 6.5 图表分类
图表分类通过关注其视觉特征和表示形式对各种图表类型进行分类。 此过程旨在准确识别图表——例如条形图、饼图、折线图、散点图和热图——无论是手动还是自动。 主要挑战在于图表类型的多样性以及它们常常微妙的视觉区别，这使得自动区分变得复杂。 准确的分类对于后续任务（例如数据挖掘和图表分析）至关重要。

AlexNet 在 2015 年 ImageNet 竞赛中的成功引发了深度学习模型在图像分类（包括图表分类）中的主导地位。 卷积神经网络 (CNN) 如 VGG、ResNet、Inception 和 EfficientNet 已被用来从图表图像中提取高级特征。 通过利用从自然图像中学习的特征来完成图表任务，迁移学习进一步提高了分类精度 [285]. ResNet 和 VGG 等网络在大型数据集和复杂图表类型上表现出色 [129, 130, 131, 132]. 随着数据集的增长和图表复杂度的增加，DenseNet 和 EfficientNet 等模型展现出更优越的性能 [133, 134, 135].

尽管取得了这些进步，但基于 CNN 的模型在处理噪声或视觉上相似的图表时仍然存在困难。 为了应对这些挑战，Vision Transformers 作为一项很有前景的发展方向应运而生。 在 2022 年的图表分类竞赛中，一个预训练的 Swin Transformer 超越了其他模型 [136]. Swin Transformer 凭借其分层结构和局部窗口注意力机制，有效地管理全局和局部图像特征，在处理复杂图表方面超越竞争对手 [134]. Swin-Chart 模型 [137] 整合了微调后的 Swin Transformer，通过权重平均策略进一步提高了性能。 此外，[138] 提出了一个由粗到细的课程学习策略，显著提高了对视觉上相似的图表的分类。

### 6.6 图表检测和元素识别
在文档中检测和分割图表需要布局检测算法来准确识别图表区域。 在提取数据之前，需要完成将图表与标题关联、分割多面板图表以及识别图表元素等任务。

#### 6.6.1 图表和标题关联
将图表与其标题链接，将图表与其文档中的标题关联起来，这对理解图表数据和实现高效检索至关重要。 基于规则的算法通常通过分析图表周围文本的空间布局来确定这种关系 [139, 140]，使用匈牙利算法和贪婪匹配等方法 [286, 287]. 最近的方法使用基于 CNN 的分类器和 OCR 通过大型数据集将标题与图表链接 [288, 289]，多模态 Transformer 结合图像和文本特征来改进标题关联 [290, 291].

#### 6.6.2 复合图表识别
复合图表在一个框架内编译多个子图表，每个子图表都有不同的数据。 对这些图表进行分割对于准确的数据提取至关重要。 早期的方法采用几何特征和基于像素轮廓的分割 [141, 292]。 将分割视为目标检测任务，诸如 YOLO 和 Faster R-CNN 等方法能够同时检测子图表及其元素 [293, 140]。

#### 6.6.3 图表元素检测
图表包含文本和视觉元素，它们对于数据提取至关重要。 关键任务包括检测文本并将其分类为标题和标签等类别。 最近用于图表中文本检测的方法通常使用半自动系统，用户输入用于识别重要的元素，例如轴标签 [288, 142, 143, 144]。 虽然传统的系统（如 Microsoft OCR 和 Tesseract OCR）在精度方面有限，但仍然被广泛使用 [142, 150]。视觉元素的检测与文本类似，深度学习模型越来越多地取代了基于规则的方法。 2023 年的上下文感知系统使用 Faster R-CNN 检测图例和数据点等元素，依赖于区域建议网络 [294]。

#### 6.6.4 文本与视觉元素之间的相关性匹配
将文本与相应的视觉元素联系起来对于解释图表数据至关重要。 早期的方法是基于规则的，侧重于位置关系 [295, 296, 297, 131]。 近期的进展，例如 2022 年推出的基于 Swin Transformer 的方法，已经改进了这些技术，通过 Transformer 架构提供了更好的相关性匹配 [136, 145]。

### 6.7 图表感知
图表感知从图表中检索数据，例如条形图、折线图、饼图和散点图。 此过程提取数据结构和文本信息。 随着视觉语言模型的发展，也可以实现端到端的图表到文本转换。

#### 6.7.1 图表数据提取
图表数据提取专注于基本图表，并已从手动方法发展到深度学习技术。 半自动系统（例如允许用户输入以提高准确性的系统）仍然很常见。 例如，[146] 允许用户指定标签和轴的位置，而 [144] 使结果更正成为可能。现代方法要么是多阶段的，要么是端到端的。 像 FigureSeer [142] 这样的两阶段模型会先对图表进行分类，然后进行目标检测。 端到端模型，例如 ChartDETR [147]，将 CNN 和 Transformer 结合起来进行元素检测，生成结构化数据。 还有一些研究利用大量图像-表格数据，通过自监督训练进行图像到文本的转换 [298, 299]。专门的方法已针对特定图表类型（如散点图、条形图和折线图）开发 [148, 149, 150, 300]。

#### 6.7.2 图表结构提取
从图表（如流程图和树状图）中提取结构信息需要检测诸如单元格框和连接线之类的组件。 关于流程图结构提取的研究集中在手绘和机器生成的图表 [301, 302]。 最近的模型，如 FR-DETR [151]，结合 DETR 和 LETR 同时检测符号和边缘，提高了准确性。 然而，仍然存在挑战，尤其是对于复杂的连接线，如 [152] 所强调的那样，该研究侧重于使用两阶段方法进行线检测的组织图表。


## 7 用于文档解析的大型模型：概述和最新进展
文档提取大型模型（DELMs）采用基于 Transformer 的架构，将多模态信息从文档（例如文本、表格和图像）中提取到结构化数据中。 与传统的基于规则的系统不同，DELMs 整合了视觉、语言和结构信息，从而能够更有效地进行文档结构分析、表格提取和跨模态关联。 这些功能使 DELMs 非常适合端到端的文档解析，有助于更深入地理解下游任务。

随着多模态大型语言模型（MLLMs），特别是视觉语言模型（LVLMs）的进步，它们在处理复杂的文档和网页等多模态输入方面越来越娴熟，这些输入结合了文本、图像和结构化信息。 尽管取得了这些进步，但在有效处理学术和专业文档方面仍然存在挑战，特别是在 OCR 和详细文档结构提取方面。 以下各节将记录 DELMs 的演变过程，重点介绍它们对这些挑战的解决方案，并说明每个模型是如何在之前的努力基础上构建的。

### 7.1 文档多模态处理的初始发展
早期的文档提取模型，例如 LLaVA-Next [156]、Qwen-VL [157] 和 InternVL [160]，旨在理解文档中的多模态内容（即图像和文本）。 这些模型通过在包含图像和文本的广泛数据集上进行训练，为大规模文档分析奠定了基础。 然而，它们的一般图像理解不足以用于更复杂的学术和专业文档，在这些文档中，特定领域的 OCR 和详细文档结构分析等任务至关重要。 这些模型能够有效地理解视觉内容，但缺乏对以文本为主的文档（例如技术报告或学术论文）所需的粒度。

为了解决这一差距，DocOwl1.5 [165] 和 Qwen-VL 等模型在特定于文档的数据集上进行了微调。 对 CLIP-ViT 架构的改进提高了它们在与文档相关的任务中的性能。 此外，TextMonkey [161] 和 Ureader [163] 等模型使用的滑动窗口等技术，有助于将大型高分辨率文档分解成更小的段，提高了 OCR 准确率。 然而，这些早期模型仍然难以对大量的文本和视觉信息进行对齐，如 GOT 模型 [6] 所示，其中对视觉推理的关注与细粒度文本提取相冲突。

### 7.2 OCR 和端到端文档解析的进展
2023 年，Nougat[170] 在文档提取方面取得了重大突破，成为第一个专为学术文档处理而设计的端到端 Transformer 模型。 Nougat 基于 Donut，并使用带有 mBART[303] 解码器的 Swin Transformer 编码器，可以直接将学术文档转换为 Markdown 格式。 这一创新集成了数学表达式识别和页面关系组织，使其特别适合科学文档。 Nougat 代表着从模块化 OCR 系统的转变，该系统分别处理文本提取、公式识别和页面格式化。 但是，它在处理非拉丁脚本方面存在局限性，并且由于计算量大，转换速度较慢。

尽管 Nougat 解决了许多先前模型的缺点，但其对学术文档的关注也留下了改进的空间，例如细粒度的 OCR 任务和图表解读。 Vary[304] 应运而生，旨在通过改进图表和文档 OCR 来应对这些挑战。 Vary 通过将 SAM 样式的视觉词汇集成到其框架中来扩展视觉词汇库，从而在不分割文档页面的情况下实现更好的 OCR 和图表理解。 但是，Vary 仍然难以处理语言多样性和多页文档，这表明仍然需要更多专门的模型。

### 7.3 处理多页文档和细粒度任务
2024 年，Fox[168] 推出了一种用于多页文档理解和细粒度重点任务的新方法。 通过利用多个预训练的视觉词汇，例如 CLIP-ViT 和 SAM 样式的 ViT，Fox 能够同时处理自然图像和文档数据，而无需修改预训练的权重。 此外，Fox 采用混合数据生成策略，综合文本和视觉元素，从而提高跨页面翻译和摘要生成等任务的性能。 该模型解决了早期 DELM 在处理复杂的多页文档结构方面遇到的局限性。

尽管 Fox 在多页文档处理方面表现出色，但其对层次化文档结构的处理方式在 Detect-Order-Construct[305] 等模型中得到了进一步的改进。 Detect-Order-Construct 引入了一种基于树构建的方法来进行层次化文档分析，将该过程分为三个阶段：检测、排序和构建。 通过检测页面对象，分配逻辑角色，并建立阅读顺序，该模型重建了整个文档的层次结构。 这种统一的关系预测方法在理解和重建复杂的文档结构方面优于传统的基于规则的方法。

### 7.4 用于文档解析和结构化数据提取的统一框架
OmniParser [306]等模型的引入标志着向统一框架的转变，该框架结合了多个文档处理任务，例如文本解析，关键信息提取和表格识别。 OmniParser 的两阶段解码器架构增强了结构信息的提取，为管理文档中的复杂关系提供了一种更可解释和更有效的方法。 通过将 OCR 与结构化序列处理解耦，OmniParser 在文本检测和表格识别方面都优于早期特定于任务的模型，如 TESTER 和 SwinTextSpotter，同时还减少了推理时间。

与此同时，GOT [6] 于 2024 年发布，通过将所有字符（文本、公式、表格、乐谱）视为对象，引入了通用 OCR 范式。 这种方法使该模型能够处理各种类型的文档，从场景文本 OCR 到细粒度文档 OCR。 GOT 使用了 500 万个文本图像对数据集及其三阶段训练策略——预训练、联合训练和微调——使其在处理复杂的图表、非传统内容（如乐谱）和几何形状方面超越了以前的特定文档模型。 GOT 代表着朝着通用 OCR 系统迈出的步伐，该系统能够解决现代文档中发现的各种内容。

总之，DELM 的演变以解决早期模型中特定限制的逐步进步为标志。 初步发展改进了多模态文档处理，而 Nougat 和 Vary 等后来的模型则改进了 OCR 功能和细粒度提取任务。 Fox 和 Detect-Order-Construct 等模型进一步完善了多页和层次文档理解。 最后，OmniParser 等统一框架和 GOT 等通用 OCR 模型为更全面、更高效和更通用的文档提取解决方案铺平了道路。 这些进步代表着分析和处理复杂文档方式的重大进步，有利于学术和专业领域。


## 8 数据集
### 8.1 单任务数据集
#### 8.1.1 DLA 数据集
文档布局分析 (DLA) 数据集主要分为合成数据集、真实世界数据集（文档和扫描图像）和混合数据集。 早期的工作集中在历史文献上，例如 IMPACT  [307]、圣加仑  [14] 和 GW20  [308]。 随着 IIT-CDIP  [309] 等包含 700 万份具有复杂布局的文档的更全面的数据集的出现，也涌现了更多的数据集。 2010 年后，研究兴趣转向了复杂的印刷布局，同时继续研究手写历史文本。 表  1 列出了过去十年 DLA 研究中使用的一些关键数据集。 此外，一些主要的会议，如国际文档分析与识别大会 (ICDAR)，举办了比赛，这些比赛引入了具有高质量、标准化标注的数据集。 这些对于模型评估和基准测试至关重要。 例如，ICDAR 2013 页面分割竞赛侧重于使用报纸、期刊和杂志进行文档布局分析，并使用多种标注类型。 ICDAR 2021 比赛强调历史文献，解决由于老化造成的布局挑战，以及为提取结构化信息而进行的科学文献解析。  

表1: DLA 常用数据集摘要  

| Dataset                 | Class        | Instance | Document Type                  | Language              |
|-------------------------|--------------|----------|---------------------------------|-----------------------|
| PRImA [310]             | 10           | 305      | Multiple Types                  | English               |
| BCE-Atabic-v1 [311]     | 3            | 1833     | Arabic books                    | Arabic                |
| Diva-hisdb [312]        | Text Block   | 150      | Handwritten Historical Document | Multiple Languages    |
| DSSE200 [313]           | 6            | 200      | Magazines, Academic papers      | English               |
| OHG [314]               | 6            | 596      | Handwritten Historical Document | English               |
| CORD [315]              | 5            | 1000     | Receipts                        | Indonesian            |
| FUNSD [316]             | 4            | 199      | Form document                   | English               |
| PubLayNet [317]         | 5            | 360000   | Academic papers                 | English               |
| Chn [318]               | 5            | 8005     | Chinese Wikipedia pages         | Chinese               |
| DocBank [319]           | 13           | 500000   | Academic papers                 | English, Chinese      |
| BCE-Atabic-v1 [320]     | 21           | 9000     | Arabic books                    | Arabic                |
| DAD [321]               | 5            | 5980     | Articles                        | English               |
| DocLayNet [322]         | 11           | 80863    | Multiple Types                  | Primarily English     |
| D4LA [27]               | 27           | 11092    | Multiple Types                  | English               |
| M6Doc [323]             | 74           | 9080     | Multiple Types                  | English, Chinese      |

#### 8.1.2 OCR 数据集
本节概述了用于印刷文本的常见 OCR 数据集。 最著名和最广泛使用的是在各种 ICDAR 竞赛中引入的数据集，例如 ICDAR2013 和 ICDAR2015，这些数据集包含真实世界的场景和文档图像，并经常被用于评估场景文本检测算法。 此外，像 Street View Text Perspective 和 MSRA-TD500 这样的数据集侧重于在具有挑战性的环境中检测不规则文本。 合成数据集，如 SynthText 和 SynthAdd，是人工生成的，包含大量数据，使其成为文本检测和识别任务的热门选择。 此外，像 ICDAR2015 和 ICDAR2019 这样的数据集提供区域标注和文本内容，支持端到端的 OCR 任务。 表 2 中总结了常用的 OCR 数据集。

表2: OCR 常用数据集总结(TD: Text Detection; TR: Text Recognition; TS: Text Spotting.)

| Dataset                             | Instance  | Task                   | Feature                        | Language             |
|-------------------------------------|-----------|------------------------|--------------------------------|----------------------|
| IIIT5K [324]                        | 5000      | TR                     | Real-world scene text          | English              |
| Street View Text [325]              | 647       | TD                     | Street View                    | English              |
| Street View Text Perspective [326]  | 645       | TD                     | Street View with perspective distortion | English              |
| ICDAR 2003 [327]                    | 507       | TD & TR                | Real-world short scene text    | English              |
| ICDAR 2013 [328]                    | 462       | TD & TR                | Real-world short scene text    | English              |
| MSRA-TD500 [329]                    | 500       | TD                     | Rotated text                   | English, Chinese     |
| CUTE80 [330]                        | 13000     | TD & TR                | Curved text                    | English              |
| COCO-Text [331]                     | 63,686    | TD & TR                | Real-world short scene text    | English              |
| ICDAR 2015 [332]                    | 1500      | TD & TR & TS            | Incidental Scene Text          | English              |
| SCUT-CTW1500 [333]                  | 1500      | TD                     | Curved text                    | English, Chinese     |
| Total-Text [334]                    | 1555      | TD & TR                | Multi-oriented scene text      | English, Chinese     |
| SynthText [335]                     | 800,000   | TD & TR                | Synthetic images               | English              |
| SynthAdd [336]                      | 1,200,000 | TD & TR                | Synthetic images               | English              |
| Occlusion Scene Text [80]           | 4832      | TD                     | Occlusion text                 | English              |
| WordArt [337]                       | 6316      | TR                     | Artistic text                  | English              |
| ICDAR2019-ReCTS [338]               | 25,000    | TD & TR & TS            | TD & TR & Document Structure Analysis | Chinese              |

#### 8.1.3 MED 和 MER 数据集
在文档分析中，数学表达式检测和识别是至关重要的研究领域。 借助专门的数据集，研究人员现在能够更好地识别各种数学表达式。 表 3 列出了用于数学表达式检测和识别的常用基准数据集，涵盖了各种文档格式（如图像和文档）中的印刷和手写数学表达式。 这些数据集支持数学表达式检测、提取、定位和数学表达式识别等任务。 重要的数据集包括 UW-III、InftyCDB-1 和 Marmot，常用于评估印刷数学表达式检测，解决内联和独立数学表达式。 ICDAR 系列通过像 ICDAR-2017 POD 和 ICDAR-2021 IBEM 这样的数据集的竞赛推动了该领域的发展，呈现了广泛而复杂的场景。 这些资源提高了识别模型的鲁棒性，并强调了在复杂的文档结构中检测数学表达式的挑战。 此外，像 FormulaNet 和 ArxivFormula 这样的数据集强调大规模检测，特别是从图像中提取数学表达式。 尽管取得了进展，但用于数学表达式检测和识别的数据集的可用性仍然有限，需要改进多格式支持和鲁棒性。

表3: MED 和 MER 常用数据集摘要

| Dataset                  | Image   | Instance  | Type                         | Task  |
|--------------------------|---------|-----------|------------------------------|-------|
| UW-III [339]             | 100     | /         | Inline and displayed Formula  | MED   |
| InftyCDB-1 [340]         | 467     | 21000     | Inline and displayed Formula  | MED   |
| Marmo [341]t             | 594     | 9500      | Inline and displayed Formula  | MED   |
| ICDAR-2017 POD [342]     | 3900    | 5400      | Only displayed Formula       | MED   |
| TFD-ICDAR 2019 [343]     | 851     | 38000     | Inline and displayed Formula  | MED   |
| ICDAR-2021 IBEM [344]    | 8900    | 166000    | Inline and displayed Formula  | MED   |
| FormulaNet [345]         | 46,672  | 1000000   | Inline and displayed Formula  | MED   |
| ArxivFormula [91]        | 700000  | 813.3     | Inline and displayed Formula  | MED   |
| Pix2tex [346]            | 189117  |           | Printed                      | MER   |
| CROHME [347]             | 12178   |           | Handwritten                  | MER   |
| HME100K [348]            | 99109   |           | Handwritten                  | MER   |
| UniMERNet [96]           | 1,061,791 |          | Printed and Handwritten      | MER   |

表4: TD 和 TSR 常用数据集摘要(TD: Table Detection; TSR: Table Structure Recognition)

| Dataset                        | Instance Count          | Type                              | Language            | Task           | Feature                                        |
|--------------------------------|-------------------------|-----------------------------------|---------------------|----------------|------------------------------------------------|
| ICDAR2013 [349]                | 150                     | Government Documents             | English             | TD & TSR       | Covers complex structures and cross-page tables|
| ICDAR2017 POD [342]            | 1548                    | Scientific papers                 | English             | TD             | Includes shape and formula detection           |
| ICDAR2019 [350]                | 2439                    | Multiple Types                    | English             | TD & TSR       | Includes historical and modern tables          |
| TABLE2LATEX-450K [124]         | 140000                  | Scientific papers                 | English             | TSR            |                                                 |
| RVL-CDIP (subset) [351]       | 518                     | Receipts                          | English             | TD             | Derived from RVL-CDIP                          |
| IIIT-AR-13K [352]             | 17,000 (not only tables)| Annual Reports                    | Multi-language      | TD             | Does not only contain tables                   |
| CamCap [353]                   | 85                      | Table images                      | English             | TD & TSR       | Used for evaluating table detection in camera-captured images |
| UNLV Table [354]               | 2889                    | Journals, Newspapers, Business Letters | English         | TD             |                                                 |
| UW-3 Table [355]              | 1,600 (around 120 tables) | Books, Magazines                | English             | TD             | Manually labeled bounding boxes                |
| Marmot [356]                   | 2000                    | Conference Papers                 | English and Chinese | TD             | Includes diversified table types; still expanding |
| TableBank [357]                | 417234                  | Multiple Types                    | English             | TD & TSR       | Automatically created by weakly supervised methods |
| DeepFigures [287]              | 5,500,000 (tables and figures) | Scientific papers           | English             | TD             | Supports figure extraction                     |
| PubTabNet [125]                | 568000                  | Scientific papers                 | English             | TSR            | Structure and content recognition of tables    |
| PubTables-1M [358]             | 1000000                 | Scientific papers                 | English             | TSR            | Evaluates the oversegmentation issue           |
| SciTSR [359]                   | 15000                   | Scientific papers                 | English             | TSR            |                                                 |
| FinTable [359]                 | 112887                  | Scientific and Financial Tables    | English             | TD & TSR       | Automatic Annotation methods                   |
| SynthTabNet [360]              | 600000                  | Multiple Types                    | English             | TD & TSR       | Synthetic tables                               |
| Wired Table in the Wild [121]  | 14582 (pages)           | Photos, Files, and Web Pages      | English             | TSR            | Deformed and occluded images                   |
| WikiTableSet [361]             | 50000000                | Wikipedia                         | English, Japanese, French | TSR       |                                                 |
| STDW [362]                     | 7000                    | Multiple Types                    | English             | TD             |                                                 |
| TableGraph-350K [363]          | 358,767                 | Academic Table                    | English             | TSR            | including TableGraph-24K                       |
| TabRecSet [364]                | 38100                   | Multiple Types                    | English and Chinese | TSR            |                                                 |
| DECO [365]                     | 1165                    | Multiple Types                    | English             | TD             | Enron document electronic table files          |
| iFLYTAB [366]                  | 17291                   | Multiple Types                    | Chinese and English | TD & TSR       | Online and offline tables from various scenarios |
| FinTab [367]                   | 1,600                   | Financial Table                    | Chinese             | TSR            |                                                 |

#### 8.1.4 TD 和 TSR 数据集
鉴于表格数据的广泛范围和复杂结构，针对表格相关任务出现了许多代表性的数据集。 具有广泛适用性的基础表格数据集主要来自官方 ICDAR 比赛，例如 ICDAR2013 和 ICDAR2017，这些比赛提供了多种来源和适当的复杂程度。 为了提高数据集中的表格多样性，研究人员引入了 TableBank，其中包含来自各个领域的高质量标注表格，包括科学文献和商业文档。 此数据集增加了表格多样性，为表格检测和识别任务提供了更广泛的应用场景和更真实的数据。 此外，TabStructDB 等数据集通过提供更详细的结构化信息（例如内部单元格表示和表格结构细节）来丰富现有数据集，从而促进更准确的结构分析。 一些数据集专门针对不规则表格样本。 例如，Marmot 数据集侧重于有线和无线表格的检测，而 CamCap 收集的是在弯曲表面拍摄的不规则表格，而 Wired Table in the Wild (WTW) 包含具有常见现实世界挑战（例如遮挡和模糊）的表格。 这些数据集提高了表格识别系统在复杂环境中的鲁棒性。 某些数据集是针对特定表格相关任务而设计的。 例如，FinTabNet 侧重于金融表格的检测和识别，而 SciTSR 专注于识别学术文章中的表格结构。 这些数据集为专业表格分析任务提供有针对性的支持，并推动细分研究领域的进展。 此外，像 WikiTableSet 和 Marmot 这样的数据集涵盖了多种语言（包括中文）的表格，有助于解决语言多样性不足的问题，并实现跨语言表格检测和结构分析。 尽管现有的表格数据集提供了丰富的数据源，并支持各种任务（如表格检测和结构识别），但它们在场景多样性、任务特异性和语言覆盖范围方面仍有提升空间。

#### 8.1.5 用于图表相关任务的数据集
文档中的图表涉及多个关键任务，包括图表分类、数据提取、结构提取和图表解释。 存在各种数据集来支持这些任务，与图表分类和信息提取相关的列在表 5 中。 图表分类领域相对成熟，拥有众多广泛使用的权威数据集。 例如，DeepChart（2015）包含 5,000 个图表，分布在五个类别中，用于分类任务。 VIEW（2012）包括 300 个图表，分布在三个类别中，专注于提高图表图像的可访问性。 ReVision（2011）包含 2,601 个实例，分布在 10 个图表类别中，能够自动对图表进行分类、分析和重新设计。 这些数据集为推进图表分类研究提供了强有力的支持。 相反，图表数据和结构提取领域通常依赖于定制数据集，例如 UB-PMC（2019-2022）和 Synth（2020）。 UB-PMC 数据集收集了来自科学出版物的真实图表，并在不同年份发布了子集。 它涵盖 4 到 15 个图表类别，实例数量从 2,000 到 30,000 不等。 Synth 2020 包含 9,600 个使用 Matplotlib 库生成的合成图表。 虽然这些数据集对于数据提取任务很有价值，但公开可用的数据源仍然有限。 在图表理解和推理领域，大型视觉语言模型的最新进展已导致高质量数据集的创建。 例如，LineEX430k (2023) 专注于折线图数据提取，包含 430,000 个折线图实例。 OneChart (2023) 是一个包含 1000 万个图表的大规模数据集，支持复杂的图表信息提取、问答和推理等任务。 这些数据集极大地促进了图表理解和推理方面的研究。

表5: 用于图表相关任务的常用数据集概述

| Dataset                          | Year | Instance   | Class           | Task                       | Feature                                             |
|----------------------------------|------|------------|-----------------|----------------------------|-----------------------------------------------------|
| DeepChart [368]                  | 2015 | 5000       | 5               | Chart Classification        |                                                     |
| VIEW [369]                       | 2012 | 300        | 3               | Chart Classification        |                                                     |
| ReVision [288]                   | 2011 | 2601       | 10              | Chart Classification        | Based on ChartSense dataset                        |
| CHART 2019 [370] - PMC           | 2019 | 4242       | multi-class     | Chart Classification        | Real charts from scientific publications            |
| CHART 2019 - Synthetic [371]     | 2019 | 202,550    | multi-class     | Chart Classification        | Synthetic charts                                    |
| DocFigure [370]                  | 2019 | 33,000     | 28              | Chart Classification        | Includes various figure images                      |
| UB-PMC 2019 [370]                | 2019 | 4242       | 7               | Chart Classification        | Competition dataset                                 |
| UB-PMC 2020 [372]                | 2020 | 2123       | 4               | Chart Data Extraction       | Real charts from PubMedCentral                      |
| UM-PMC 2021 [373]                | 2021 | 22,924     | 15              | Chart Classification        | Competition dataset                                 |
| UB-PMC 2022 [136]                | 2022 | 33,186     | 15              | Chart Classification        | Competition dataset                                 |
| Synth 2020 [373]                 | 2020 | 9,600      | 4               | Chart Data Extraction       | Synthetic charts                                    |
| LINEEX430k [300]                 | 2023 | 430,000    | Line charts     | Chart Data Extraction       | Focused on line charts                              |
| ICPR 2022 [136]                  | 2022 | 26,596     | 15              | Chart Classification        | Charts with embedded text                          |
| ExcelChart400K [374]             | 2021 | 400,000    | Pie and bar charts | Chart Data Extraction     | Extracted from Excel charts with JSON annotations  |
| CHARTER [375]                    | 2021 | 32,334     | 4               | Chart Data Extraction       | Sourced from document pages, web pages, PubMed, FigureQA, etc. |
| StructChart dataset [376]        | 2023 | 16,466     | Organization and structure charts | Chart Structure Extraction |                                                     |
| OneChart [377]                   | 2023 | 10,000,000 | 5               | Chart Information Extraction, QA, and Inference | Synthesized using Matplotlib                   |
| Chart-to-Text [378]              | 2023 | 8,305      | 6               | Chart Information Extraction | Contains chart samples and corresponding data      |
| ChartLlama [379]                 | 2023 | 1,500      | 10              | 7 comprehensive chart tasks | GPT-4 generates charts and instruction data        |
| ChartX [299]                     | 2024 | 48,000     | 18              | 7 comprehensive chart tasks | Automatically generated by GPT-4 and manually checked |

#### 8.2其他用于文档相关任务的数据集
除了面向特定任务的数据集，还有其他支持多个文档相关任务的数据集。 一个值得注意的例子是 FUNSD 数据集，它被广泛应用于表格处理和文档 OCR。 尽管规模较小，只有 199 张图像，但它为通过标注的文本块和关系解析表格和文档结构提供了宝贵的资源。 这个数据集非常适合结构化文档理解的早期训练和测试。

相反，SROIE 数据集  [380] 包含 1,000 张收据图像，专注于提取关键信息，如公司名称、日期、地址和总成本，使其特别适合于自动化金融和零售行业的文档信息提取。

LOCR 数据集  [74] 专注于学术文档任务，是一个从 arXiv 学术论文中提取的大规模资源。 它涵盖了 125,738 页，包含超过 7700 万个文本位置对，为复杂的排版元素提供了标注的边界框，使其适用于需要识别复杂布局的学术 OCR 任务。

另一个宝贵的资源，DocGenome 数据集  [5] 是一个开源的大规模基准，包含来自 arXiv 的 500,000 个科学文档，涵盖 153 个学科和 13 个文档组件（例如图表、数学表达式、表格）。 该数据集使用 DocParser 标注工具创建，支持多模态任务，如文档分类、布局检测和视觉定位，以及文档组件转换为 LaTeX。 该数据集旨在评估和训练用于文档理解任务的大型多模态模型。

OCRBench  [381] 作为一个综合的评估平台，整合了 29 个数据集，涵盖了各种与 OCR 相关的任务，例如文本识别、视觉问答和手写数学表达式识别。 它突出了 OCR 任务的复杂性以及多模态模型在跨任务性能方面的潜力。

在专业领域，CHEMU  [382] 和 ChEMBL25  [383] 数据集专注于识别化学文献中的分子数学表达式和化学结构，从而将 OCR 应用扩展到科学符号提取和分析。 MUSCIMA++  [384] 和 DeepScores  [385] 针对音乐记谱 OCR，通过标注手写音乐记谱和符号，从而推动音乐符号识别。 这些数据集说明了 OCR 在高度技术领域中的潜力和挑战。

大型文档模型数据集的最新发展为文档解析和大型模型训练开辟了新的途径。 例如，Nougat 利用来自 arXiv、PubMed Central (PMC) 和工业文档库 (IDL) 的数据集，通过将文档页面与源代码配对来构建，特别是为了在数学表达式和表格中保留语义信息。

Vary 数据集包括 200 万中文和英文文档图像-文本对，150 万图表图像-文本对和 12 万自然图像负样本对。 该数据集将新的视觉词汇与 CLIP 词汇合并，使其适用于中文和英文环境中的 OCR、Markdown/LaTeX 转换和图表理解等任务。

GOT 模型数据集包含约 500 万个图像-文本对，这些图像-文本对来自 Laion-2B、Wukong 和 Common Crawl，涵盖中英文数据。 它包括 200 万个场景文本数据点和 300 万个文档级数据点，合成数据集支持音乐记谱识别、分子数学表达式、几何图形和图表分析等任务。 这种多样性使 GOT 能够解决各种 OCR 任务，从通用文档 OCR 到专业和细粒度的 OCR。

文档解析数据集的多样性和复杂性推动了文档相关算法和大型模型的进步。 这些数据集为模型提供了广泛的测试平台，并为跨各个领域的文档处理提供了新的解决方案。


## 9 评估指标
### 9.1 DLA 指标
在文档布局检测中，结果通常包括文档元素的坐标区域信息和分类。 因此，文档布局分析 (DLA) 的评估指标强调元素位置识别的准确性、识别准确性以及结构层次的重要性，以全面反映模型在分割、识别和重建文档结构方面的性能。 对于元素位置识别的准确性，主要使用交并比 (IoU) 来衡量预测框和实际框之间的重叠程度。 关于模型识别准确性，常用的指标包括精确率、召回率和 F1 分数。 除了上述传统评估指标外，还可以根据具体的分析目标灵活调整。 在接下来的部分中，对于文本检测、数学表达式、表格检测等，主要使用精确率、召回率、F1 分数和 IoU 等指标进行评估，因此不会提供详细的介绍。

表格6：DLA 常用指标  
![表格6：DLA 常用指标](/img/article-img/2024/11/1106_8.jpg)

### 9.2 OCR 指标
文本检测和文本识别是 OCR 任务中的两个关键步骤，每个步骤都有不同的评估指标。 文本检测更侧重于定位精度和覆盖范围，主要使用精确率、召回率、F1 分数和 IoU 来评估性能。 相反，文本识别强调识别结果的正确性，通常使用字符错误率、词错误率、编辑距离和 BLEU 分数进行评估。 在 LOCR [74] 等项目中，还引入了 METEOR 来弥补 BLEU 的一些不足，为机器生成文本和参考文本之间的相似度提供更全面的评估。

表格7：OCR 常用指标  
![表格7：OCR 常用指标](/img/article-img/2024/11/1106_9.jpg)

### 9.3 MER 指标
虽然数学表达式可以通过将它们转换为格式化的代码后使用 OCR 任务指标进行评估，但 BLEU、编辑距离和 ExpRate 是当前数学表达式识别领域中最常用的评估指标，每个指标都有其自身的局限性。 由于数学表达式可以有多种有效的表示形式，因此仅仅依赖于文本匹配的指标无法公平且准确地评估识别结果。 一些研究试图将图像评估指标应用于数学表达式识别，但结果并不理想  [386]。评估数学表达式识别的结果仍然需要进一步探索和发展。   [387] 提出了字符检测匹配 (CDM)，这是一种消除由于不同 LaTeX 表示而产生的问题的指标，提供了一种更直观、准确和公平的评估方法。

表格 8: MER 常用指标  
![表格 8: MER 常用指标](/img/article-img/2024/11/1106_10.jpg)

### 9.4 表格识别指标
在表格检测任务中，除了常见的字符级召回率、精确度和 F1 分数外，纯度和完整性也可以用于检测。 表格结构识别主要关注分析表格内部的布局结构以及单元格之间的关系。 除了传统的精度和召回率等指标外，最近开发的详细评估方法为评估表格识别任务提供了更多维度，例如行和列准确率、多列召回率 (MCR) 和多行召回率 (MRR)  [388]。 随着表格识别领域不断发展，也提出了一些通用的评估指标，例如单元格邻接关系 (CAR) 和基于树编辑距离的相似性 (TEDS)[125]。   [128] 引入了一个简化版本的 S-TEDS 指标，该指标只考虑表格的逻辑结构，忽略单元格内容，并关注行、列、跨行和跨列信息的匹配。 TGRNet  [152] 中的性能评估指标提供了几个创新想法，提出了 Aall 等指标，该指标同时描述了四个逻辑位置，以及 Fβ，该指标衡量了综合性能。 它还使用加权平均 F 分数来评估在不同 IoU 阈值下邻接关系预测的性能。 涉及将表格转换为 LaTeX 或其他结构化语言的任务，字符级评估通常是主要的评估方法。 字母数字符号评估 (AN) 评估了模型生成的结构化代码与基本事实中的字母数字符号之间的匹配程度。 LaTeX 符元和非 LaTeX 符号评估 (LT) 衡量模型在生成 LaTeX 特定符号方面的准确性。 此外，平均莱文斯坦距离 (ALD) 计算生成的结构化代码与真实值之间的编辑距离，量化两个字符串之间的相似性。 由于表格检测和识别任务的特殊性，存在多种评估指标。 许多研究根据自身需求提出了具有特定关注点的不同指标。 使用多种指标的组合可以更全面地评估模型的性能。 随着任务复杂性的增加，未来的评估工作可能更多地依赖于细粒度的评估指标。

表格9: 表格检测和识别常用的指标  
![表格9: 表格检测和识别常用的指标 ](/img/article-img/2024/11/1106_11.jpg)

9.5图表相关任务的指标
在图表分类中，评估指标与标准分类任务中的指标类似，因此我们不会在此详细介绍。 对于图表元素检测，通常使用平均 IoU、召回率和精确率等指标来评估元素（例如，文本区域、条形）的检测 [373]。 此外，对于数据转换，使用诸如 s0（视觉元素检测得分）、s1（图例匹配准确性的平均名称得分）、s2（数据转换准确性的平均数据系列得分）和 s3（所有指标的综合得分）等指标。 这些指标全面评估了数据提取框架对各种类型图表数据的有效性和稳健性。

从图表中提取数据和结构的任务仍处于发展不足阶段，没有建立标准的评估指标。 例如，在 ChartOCR 项目中，使用自定义指标来评估不同类型的图表，例如条形图、饼图和折线图。 条形图评估使用预测边界框和真实边界框之间的距离函数，得分从解决分配问题中得出。 对于饼图，数据值的重要性顺序在序列匹配框架中进行考虑，分数通过动态规划计算。 ChartDETR 使用精确率、召回率和 F1 分数。

对于折线图，使用严格和松散的对象关键点相似度指标，提供一个平衡的视角，融合了准确性和灵活性。 此方法也被 LINEEX 采用。

对于具有结构关系的图表（例如，树形图），结构化数据提取评估器会修改现有的指标。 例如，在 [152] 中，只有当所有组件被准确提取时，诸如所有权或从属关系之类的元组才被认为是正确的，并且计算诸如精确率、召回率和 F1 分数之类的指标。

StructChart [376] 引入了面向结构化图表的表示指标（SCRM）来评估图表感知任务。 SCRM 包括在固定相似度阈值下的精确率和跨不同阈值的平均精确率（mPrecision）。 公式如下：

$$ \mathrm{Precision}_{\mathrm{IOU_{thr,tol}}}=\frac{\sum_{i\,=\,1}^{L}d(i)_{\mathrm{IOU_{thr},tol}}}{L} $$

$$m\mathrm{Precision}_{\mathrm{tol}}=\frac{\sum_{i\,=\,10}^{19}\,\sum_{i\,=\,1}^{L}d(i,0.05t)_{\mathrm{tol}}}{10L}$$

这里，L 表示图像总数，$d(i)_{\mathrm{IoU_{thr,tol}}}$ 是判别函数，如果第 i 个图像的 IoU 在容差范围内满足阈值，则输出 1；否则，输出 0。 同样，$d(i,0.05t)_{\mathrm{tol}}$ 是另一个判别函数，用于从 0.5 到 0.95 的不同阈值 t 。

总之，图表数据和结构提取任务由于多样化和复杂的评估标准，展现出巨大的发展机会。 随着研究的进展，建立一个全面且普遍适用的图表提取评估系统变得越来越必要。

表10： 用于文档上下文提取的开源工具

| Tools         | Developer      | Time        | Introduction                                                                                                    |
|--------------|---------------|-------------|----------------------------------------------------------------------------------------------------------------|
| GROBID        | Patrice Lopez  | 2011        | A machine learning library that focuses on extracting and restructuring original documents, converting them into structured formats such as XML/TEI encoding. |
| PyMuPDF       | Jorj X. McKie  | 2011        | A Python library for extracting, analyzing, converting, and processing data from PDFs and other documents, supporting tables, figures, and other types of content. |
| doc2text      | Joe Sutherland | 2016.9      | Specializes in extracting low-quality documents; only ensures compatibility in Linux.                                           |
| pdfplumber    | Jeremy Singer-Vine | 2019.1    | Tools for extraction and parsing of characters, images, lines, tables, and other elements from digital PDF documents.               |
| Parsr         | axa-group      | 2019.8      | A tool for cleaning, parsing, and extracting content from various document types, with outputs including JSON, Markdown, CSV/pandasDF, and txt formats. |
| PP-StructureV2 | Baidu         | 2021.8      | Intelligent document analysis system, supports layout analysis of Chinese and English documents, table recognition, and semantic recognition.          |
| DocxChain     | Alibaba       | 2023.9      | A system for non-structured or semi-structured document conversion into various information and formats, including complex document applications based on computational capabilities. |
| pdf2htmlEX    | Lu Wang        | 2023.12     | A project to convert PDF documents into HTML format.                                                                    |
| MinerU        | OpenDataLab    | 2024.4      | A system for extracting content from PDF and converting it into markdown or JSON formats.                                        |
| PDF-Extract-Kit | OpenDataLab    | 2024.7      | A system based on MinerU to extract various content from PDF, including layout analysis, OCR, table recognition, and formula recognition tasks.   |
| OmniParser    | Adithya S Kolavi | 2024.6      | A platform for extracting and parsing any unstructured data, transforming it into structured, actionable data optimized for GenAI applications. |
| LLM_aided_ocr  | Jeff Emanuel   | 2024.8      | Uses Tesseract for document OCR, followed by LLM-based error correction, with final output in markdown or similar formats.                |


## 10 用于文档提取的开源工具
表格 10 列出了几个在 GitHub 上拥有超过 1,000 颗星的开源文档提取工具，这些工具旨在处理各种文档格式和转换任务。

光学字符识别 (OCR) 是文档处理和内容提取的关键方面。 它使用计算机视觉技术来检测和提取文档中的字符和文本，将图像转换为可编辑和可搜索的数据。 现代 OCR 工具在准确性、速度和多语言支持方面有了显著改进。 广泛使用的通用 OCR 系统，如 Tesseract 和 PaddleOCR，对该领域做出了重大贡献。 Tesseract 是一款开源引擎，提供强大的文本识别功能和灵活的配置，使其特别适合大规模文本提取任务。 PaddleOCR 以其强大的多语言功能，在准确性和速度方面表现出色，尤其是在复杂场景中。

虽然像 Tesseract 和 PaddleOCR 这样的通用工具对于文档 OCR 非常有效，但像 Unstructured 和 Zerox 这样的专用工具在处理复杂文档结构（如嵌套表格或包含文本和图像的文档）方面表现出色。 这些工具特别擅长提取结构化信息。

除了 OCR 之外，大型模型越来越多地被用于文档解析。 最近的模型，如 Nougat、Fox、Vary 和 GOT，在处理复杂文档方面表现出色，尤其是在 PDF 格式方面。 例如，Nougat 专为解析科学文档而设计，擅长提取公式和符号。 Fox 整合了多模态信息，提高了其在语义理解和信息检索方面的有效性。 Vary 专注于解析各种文档格式，包括那些包含嵌入图像、文本框和表格的格式。 GOT 是 OCR 2.0 时代领先的模型，它使用统一的端到端架构，具有先进的视觉感知能力，使其能够处理各种内容，例如文本、表格、数学公式、分子结构和几何图形。 此外，区域级别的 OCR、高分辨率处理以及多页文档的批处理操作。

此外，在图像和语言任务中常用的大型多模态模型，例如 GPT-4、QwenVL、InternVL 和 LLaMA 系列，也能在一定程度上执行文档解析。


## 11 讨论
用于文档解析的模块化文档解析系统和视觉-语言模型 (VLMs) 仍然面临着一些挑战和局限性。

**基于管道的系统的挑战和未来方向。**  
基于管道的文档解析系统面临着关键挑战，例如集成多个模块、标准化输出格式以及处理复杂文档中不规则的阅读顺序。 例如，像 MinerU 这样的系统需要对输入文档进行密集的预处理、复杂的后期处理以及对每个模块进行专门的训练，才能达到预期结果。 此外，关于文档阅读顺序的研究仍然有限，许多方法仍然依赖于对复杂布局（例如多列格式）难以处理的规则。 管道系统通常逐页处理文档，进一步限制了它们的便利性。

这些系统的有效性也高度依赖于各个模块的性能；因此，每个模块的进步对于整个系统改进至关重要。 尽管取得了一些进展，但各个模块中仍然存在一些特定的挑战：

 - 布局分析 (DLA)：分析具有嵌套元素的复杂文档布局的准确性需要提高。 未来的 DLA 技术应侧重于集成语义信息，以增强对细粒度布局的理解，例如多级标题结构。
 - 文档 OCR：当前系统难以准确识别大块密集排列的文本，并处理多种字体格式，例如粗体和斜体。 此外，在通用 OCR 任务和专门任务（如表格识别）之间取得平衡仍然是一个问题。
 - 表格检测和识别：表格的形状会显著影响检测性能。 例如，检测没有清晰边框或跨越多个页面的表格仍然具有挑战性。 在识别方面，处理嵌套表格、没有单元格边框的表格以及包含多行文本的单元格仍然需要改进。
 - 数学表达式识别：检测和识别文档中的内联和多行数学表达式仍然很困难。 对于印刷的数学表达式，结构提取需要改进，而处理跨不同字体大小、噪声和失真的屏幕截图的数学表达式的鲁棒性也需要关注。 手写数学表达式带来了额外的挑战。 此外，当前用于数学表达式识别的评估指标不足，需要更细致、一致的基准。
 - 图表提取：从文档中提取图表是一个不断发展的领域，但缺乏统一的定义和标准转换范式。 现有方法通常是半自动化的，或者为特定类型的图表而设计，存在应用局限性。 端到端模型很有前景，但需要在识别图表元素、OCR和理解图表结构方面进行改进。 虽然当前的多模态大型语言模型 (MLLMs) 在处理复杂图表类型方面具有巨大潜力，但它们难以集成到模块化文档解析系统中。

**大型视觉模型的挑战和未来方向。**  
相反，用于文档解析的大型视觉模型通过提供端到端解决方案，消除了对复杂模块连接和后处理的需求，从而提供了显著的优势。 它们还解决了一些基于流水线的系统在理解文档结构和生成具有更高语义一致性的输出方面的局限性。 然而，大型模型并非没有挑战。

 - 性能局限性：最值得注意的是，用于文档解析的大型模型并不总是优于模块化系统，特别是在区分页眉页脚等页面元素或处理高密度文本和复杂表格结构方面。 这部分是由于缺乏针对涉及复杂文档和高分辨率图像的任务的微调模型。
 - 冻结参数和 OCR 功能：特别是，许多 LVM 在训练期间冻结 LLM 参数，这会阻碍它们在处理大量文本时的光学字符识别 (OCR) 功能。 尽管当前模型在对文档图像进行编码方面表现出色，但在长文档生成中，重复输出和格式错误等挑战仍然存在。 这些问题可以通过开发更好的解码策略或采用正则化技术来缓解。
 - 资源效率：大型模型的训练和部署成本也很高，它们在处理高密度文本方面的效率低下会导致大量的资源浪费。 在处理大量文本时，现有的图像和文本特征对齐方法不足，尤其是在 A4 尺寸文档等密集文档格式中。 虽然大型模型本质上需要大量的参数，但通过架构改进和数据增强进行优化可以帮助减小模型尺寸。

除了技术挑战之外，当前的文档解析研究通常侧重于结构化文档类型，例如科学论文和教科书，而对说明书、海报和报纸等更复杂的文档则探索不足。 这种狭隘的关注限制了该领域整体的发展。 需要更大、更丰富的数据集来支持训练和评估工作。


## 12 结论
本文详细概述了文档解析，涵盖了模块化系统和大型模型。 它回顾了数据集、评估指标和开源工具，并强调了该领域的当前局限性。 文档解析技术因其广泛的应用而受到越来越多的关注，例如检索增强生成 (RAG)、信息存储以及作为训练数据的来源。 虽然模块化系统被广泛使用，但端到端大型模型在未来发展中显示出巨大潜力。 未来，文档解析有望发展成为一种更准确、多语言且支持下游任务的友好型技术，支持各种 OCR 任务。


## 参考文献
具体信息请参考 [**原论文**](https://arxiv.org/pdf/2410.21169) 。

